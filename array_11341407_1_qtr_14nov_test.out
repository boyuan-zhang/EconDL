/lustre06/project/6019022/VARNN-Isaac-Latest/ENV/lib/python3.7/site-packages/statsmodels/tsa/ar_model.py:252: FutureWarning: The parameter names will change after 0.12 is released. Set old_names to False to use the new names now. Set old_names to True to use the old names. 
  FutureWarning,
/lustre06/project/6019022/VARNN-Isaac-Latest/EconDL/EconDL/Benchmarks.py:62: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  mat_data_d[f'{col}.l{lag}'] = mat_data_d[col].shift(lag)
/lustre06/project/6019022/VARNN-Isaac-Latest/EconDL/EconDL/Benchmarks.py:188: RuntimeWarning: Mean of empty slice
  preds_train_in = np.nanmean(preds_train_in_all, axis = 2)
/lustre06/project/6019022/VARNN-Isaac-Latest/EconDL/EconDL/Benchmarks.py:191: RuntimeWarning: Mean of empty slice
  betas = np.nanmean(betas_all, axis = 3)
/lustre06/project/6019022/VARNN-Isaac-Latest/EconDL/EconDL/Benchmarks.py:188: RuntimeWarning: Mean of empty slice
  preds_train_in = np.nanmean(preds_train_in_all, axis = 2)
/lustre06/project/6019022/VARNN-Isaac-Latest/EconDL/EconDL/Benchmarks.py:191: RuntimeWarning: Mean of empty slice
  betas = np.nanmean(betas_all, axis = 3)
/lustre06/project/6019022/VARNN-Isaac-Latest/ENV/lib/python3.7/site-packages/statsmodels/tsa/ar_model.py:252: FutureWarning: The parameter names will change after 0.12 is released. Set old_names to False to use the new names now. Set old_names to True to use the old names. 
  FutureWarning,
/lustre06/project/6019022/VARNN-Isaac-Latest/ENV/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.
  UserWarning,
/lustre06/project/6019022/VARNN-Isaac-Latest/ENV/lib/python3.7/site-packages/sklearn/ensemble/_forest.py:554: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.
  UserWarning,
Device cuda
Folder made at results/qtr_14nov_test
# Experiments: 7, # Instances: 432
Instance to experiment mapping: {0: {'exp': 0, 'reestim': 0, 'repeat': 0}, 1: {'exp': 0, 'reestim': 0, 'repeat': 1}, 2: {'exp': 0, 'reestim': 0, 'repeat': 2}, 3: {'exp': 0, 'reestim': 1, 'repeat': 0}, 4: {'exp': 0, 'reestim': 1, 'repeat': 1}, 5: {'exp': 0, 'reestim': 1, 'repeat': 2}, 6: {'exp': 0, 'reestim': 2, 'repeat': 0}, 7: {'exp': 0, 'reestim': 2, 'repeat': 1}, 8: {'exp': 0, 'reestim': 2, 'repeat': 2}, 9: {'exp': 0, 'reestim': 3, 'repeat': 0}, 10: {'exp': 0, 'reestim': 3, 'repeat': 1}, 11: {'exp': 0, 'reestim': 3, 'repeat': 2}, 12: {'exp': 0, 'reestim': 4, 'repeat': 0}, 13: {'exp': 0, 'reestim': 4, 'repeat': 1}, 14: {'exp': 0, 'reestim': 4, 'repeat': 2}, 15: {'exp': 0, 'reestim': 5, 'repeat': 0}, 16: {'exp': 0, 'reestim': 5, 'repeat': 1}, 17: {'exp': 0, 'reestim': 5, 'repeat': 2}, 18: {'exp': 0, 'reestim': 6, 'repeat': 0}, 19: {'exp': 0, 'reestim': 6, 'repeat': 1}, 20: {'exp': 0, 'reestim': 6, 'repeat': 2}, 21: {'exp': 0, 'reestim': 7, 'repeat': 0}, 22: {'exp': 0, 'reestim': 7, 'repeat': 1}, 23: {'exp': 0, 'reestim': 7, 'repeat': 2}, 24: {'exp': 0, 'reestim': 8, 'repeat': 0}, 25: {'exp': 0, 'reestim': 8, 'repeat': 1}, 26: {'exp': 0, 'reestim': 8, 'repeat': 2}, 27: {'exp': 1, 'reestim': 0, 'repeat': 0}, 28: {'exp': 1, 'reestim': 0, 'repeat': 1}, 29: {'exp': 1, 'reestim': 0, 'repeat': 2}, 30: {'exp': 1, 'reestim': 1, 'repeat': 0}, 31: {'exp': 1, 'reestim': 1, 'repeat': 1}, 32: {'exp': 1, 'reestim': 1, 'repeat': 2}, 33: {'exp': 1, 'reestim': 2, 'repeat': 0}, 34: {'exp': 1, 'reestim': 2, 'repeat': 1}, 35: {'exp': 1, 'reestim': 2, 'repeat': 2}, 36: {'exp': 1, 'reestim': 3, 'repeat': 0}, 37: {'exp': 1, 'reestim': 3, 'repeat': 1}, 38: {'exp': 1, 'reestim': 3, 'repeat': 2}, 39: {'exp': 1, 'reestim': 4, 'repeat': 0}, 40: {'exp': 1, 'reestim': 4, 'repeat': 1}, 41: {'exp': 1, 'reestim': 4, 'repeat': 2}, 42: {'exp': 1, 'reestim': 5, 'repeat': 0}, 43: {'exp': 1, 'reestim': 5, 'repeat': 1}, 44: {'exp': 1, 'reestim': 5, 'repeat': 2}, 45: {'exp': 1, 'reestim': 6, 'repeat': 0}, 46: {'exp': 1, 'reestim': 6, 'repeat': 1}, 47: {'exp': 1, 'reestim': 6, 'repeat': 2}, 48: {'exp': 1, 'reestim': 7, 'repeat': 0}, 49: {'exp': 1, 'reestim': 7, 'repeat': 1}, 50: {'exp': 1, 'reestim': 7, 'repeat': 2}, 51: {'exp': 1, 'reestim': 8, 'repeat': 0}, 52: {'exp': 1, 'reestim': 8, 'repeat': 1}, 53: {'exp': 1, 'reestim': 8, 'repeat': 2}, 54: {'exp': 2, 'reestim': 0, 'repeat': 0}, 55: {'exp': 2, 'reestim': 0, 'repeat': 1}, 56: {'exp': 2, 'reestim': 0, 'repeat': 2}, 57: {'exp': 2, 'reestim': 1, 'repeat': 0}, 58: {'exp': 2, 'reestim': 1, 'repeat': 1}, 59: {'exp': 2, 'reestim': 1, 'repeat': 2}, 60: {'exp': 2, 'reestim': 2, 'repeat': 0}, 61: {'exp': 2, 'reestim': 2, 'repeat': 1}, 62: {'exp': 2, 'reestim': 2, 'repeat': 2}, 63: {'exp': 2, 'reestim': 3, 'repeat': 0}, 64: {'exp': 2, 'reestim': 3, 'repeat': 1}, 65: {'exp': 2, 'reestim': 3, 'repeat': 2}, 66: {'exp': 2, 'reestim': 4, 'repeat': 0}, 67: {'exp': 2, 'reestim': 4, 'repeat': 1}, 68: {'exp': 2, 'reestim': 4, 'repeat': 2}, 69: {'exp': 2, 'reestim': 5, 'repeat': 0}, 70: {'exp': 2, 'reestim': 5, 'repeat': 1}, 71: {'exp': 2, 'reestim': 5, 'repeat': 2}, 72: {'exp': 2, 'reestim': 6, 'repeat': 0}, 73: {'exp': 2, 'reestim': 6, 'repeat': 1}, 74: {'exp': 2, 'reestim': 6, 'repeat': 2}, 75: {'exp': 2, 'reestim': 7, 'repeat': 0}, 76: {'exp': 2, 'reestim': 7, 'repeat': 1}, 77: {'exp': 2, 'reestim': 7, 'repeat': 2}, 78: {'exp': 2, 'reestim': 8, 'repeat': 0}, 79: {'exp': 2, 'reestim': 8, 'repeat': 1}, 80: {'exp': 2, 'reestim': 8, 'repeat': 2}, 81: {'exp': 2, 'reestim': 9, 'repeat': 0}, 82: {'exp': 2, 'reestim': 9, 'repeat': 1}, 83: {'exp': 2, 'reestim': 9, 'repeat': 2}, 84: {'exp': 2, 'reestim': 10, 'repeat': 0}, 85: {'exp': 2, 'reestim': 10, 'repeat': 1}, 86: {'exp': 2, 'reestim': 10, 'repeat': 2}, 87: {'exp': 2, 'reestim': 11, 'repeat': 0}, 88: {'exp': 2, 'reestim': 11, 'repeat': 1}, 89: {'exp': 2, 'reestim': 11, 'repeat': 2}, 90: {'exp': 2, 'reestim': 12, 'repeat': 0}, 91: {'exp': 2, 'reestim': 12, 'repeat': 1}, 92: {'exp': 2, 'reestim': 12, 'repeat': 2}, 93: {'exp': 2, 'reestim': 13, 'repeat': 0}, 94: {'exp': 2, 'reestim': 13, 'repeat': 1}, 95: {'exp': 2, 'reestim': 13, 'repeat': 2}, 96: {'exp': 2, 'reestim': 14, 'repeat': 0}, 97: {'exp': 2, 'reestim': 14, 'repeat': 1}, 98: {'exp': 2, 'reestim': 14, 'repeat': 2}, 99: {'exp': 2, 'reestim': 15, 'repeat': 0}, 100: {'exp': 2, 'reestim': 15, 'repeat': 1}, 101: {'exp': 2, 'reestim': 15, 'repeat': 2}, 102: {'exp': 2, 'reestim': 16, 'repeat': 0}, 103: {'exp': 2, 'reestim': 16, 'repeat': 1}, 104: {'exp': 2, 'reestim': 16, 'repeat': 2}, 105: {'exp': 2, 'reestim': 17, 'repeat': 0}, 106: {'exp': 2, 'reestim': 17, 'repeat': 1}, 107: {'exp': 2, 'reestim': 17, 'repeat': 2}, 108: {'exp': 2, 'reestim': 18, 'repeat': 0}, 109: {'exp': 2, 'reestim': 18, 'repeat': 1}, 110: {'exp': 2, 'reestim': 18, 'repeat': 2}, 111: {'exp': 2, 'reestim': 19, 'repeat': 0}, 112: {'exp': 2, 'reestim': 19, 'repeat': 1}, 113: {'exp': 2, 'reestim': 19, 'repeat': 2}, 114: {'exp': 2, 'reestim': 20, 'repeat': 0}, 115: {'exp': 2, 'reestim': 20, 'repeat': 1}, 116: {'exp': 2, 'reestim': 20, 'repeat': 2}, 117: {'exp': 2, 'reestim': 21, 'repeat': 0}, 118: {'exp': 2, 'reestim': 21, 'repeat': 1}, 119: {'exp': 2, 'reestim': 21, 'repeat': 2}, 120: {'exp': 2, 'reestim': 22, 'repeat': 0}, 121: {'exp': 2, 'reestim': 22, 'repeat': 1}, 122: {'exp': 2, 'reestim': 22, 'repeat': 2}, 123: {'exp': 2, 'reestim': 23, 'repeat': 0}, 124: {'exp': 2, 'reestim': 23, 'repeat': 1}, 125: {'exp': 2, 'reestim': 23, 'repeat': 2}, 126: {'exp': 2, 'reestim': 24, 'repeat': 0}, 127: {'exp': 2, 'reestim': 24, 'repeat': 1}, 128: {'exp': 2, 'reestim': 24, 'repeat': 2}, 129: {'exp': 2, 'reestim': 25, 'repeat': 0}, 130: {'exp': 2, 'reestim': 25, 'repeat': 1}, 131: {'exp': 2, 'reestim': 25, 'repeat': 2}, 132: {'exp': 2, 'reestim': 26, 'repeat': 0}, 133: {'exp': 2, 'reestim': 26, 'repeat': 1}, 134: {'exp': 2, 'reestim': 26, 'repeat': 2}, 135: {'exp': 2, 'reestim': 27, 'repeat': 0}, 136: {'exp': 2, 'reestim': 27, 'repeat': 1}, 137: {'exp': 2, 'reestim': 27, 'repeat': 2}, 138: {'exp': 2, 'reestim': 28, 'repeat': 0}, 139: {'exp': 2, 'reestim': 28, 'repeat': 1}, 140: {'exp': 2, 'reestim': 28, 'repeat': 2}, 141: {'exp': 2, 'reestim': 29, 'repeat': 0}, 142: {'exp': 2, 'reestim': 29, 'repeat': 1}, 143: {'exp': 2, 'reestim': 29, 'repeat': 2}, 144: {'exp': 2, 'reestim': 30, 'repeat': 0}, 145: {'exp': 2, 'reestim': 30, 'repeat': 1}, 146: {'exp': 2, 'reestim': 30, 'repeat': 2}, 147: {'exp': 2, 'reestim': 31, 'repeat': 0}, 148: {'exp': 2, 'reestim': 31, 'repeat': 1}, 149: {'exp': 2, 'reestim': 31, 'repeat': 2}, 150: {'exp': 2, 'reestim': 32, 'repeat': 0}, 151: {'exp': 2, 'reestim': 32, 'repeat': 1}, 152: {'exp': 2, 'reestim': 32, 'repeat': 2}, 153: {'exp': 2, 'reestim': 33, 'repeat': 0}, 154: {'exp': 2, 'reestim': 33, 'repeat': 1}, 155: {'exp': 2, 'reestim': 33, 'repeat': 2}, 156: {'exp': 2, 'reestim': 34, 'repeat': 0}, 157: {'exp': 2, 'reestim': 34, 'repeat': 1}, 158: {'exp': 2, 'reestim': 34, 'repeat': 2}, 159: {'exp': 2, 'reestim': 35, 'repeat': 0}, 160: {'exp': 2, 'reestim': 35, 'repeat': 1}, 161: {'exp': 2, 'reestim': 35, 'repeat': 2}, 162: {'exp': 3, 'reestim': 0, 'repeat': 0}, 163: {'exp': 3, 'reestim': 0, 'repeat': 1}, 164: {'exp': 3, 'reestim': 0, 'repeat': 2}, 165: {'exp': 3, 'reestim': 1, 'repeat': 0}, 166: {'exp': 3, 'reestim': 1, 'repeat': 1}, 167: {'exp': 3, 'reestim': 1, 'repeat': 2}, 168: {'exp': 3, 'reestim': 2, 'repeat': 0}, 169: {'exp': 3, 'reestim': 2, 'repeat': 1}, 170: {'exp': 3, 'reestim': 2, 'repeat': 2}, 171: {'exp': 3, 'reestim': 3, 'repeat': 0}, 172: {'exp': 3, 'reestim': 3, 'repeat': 1}, 173: {'exp': 3, 'reestim': 3, 'repeat': 2}, 174: {'exp': 3, 'reestim': 4, 'repeat': 0}, 175: {'exp': 3, 'reestim': 4, 'repeat': 1}, 176: {'exp': 3, 'reestim': 4, 'repeat': 2}, 177: {'exp': 3, 'reestim': 5, 'repeat': 0}, 178: {'exp': 3, 'reestim': 5, 'repeat': 1}, 179: {'exp': 3, 'reestim': 5, 'repeat': 2}, 180: {'exp': 3, 'reestim': 6, 'repeat': 0}, 181: {'exp': 3, 'reestim': 6, 'repeat': 1}, 182: {'exp': 3, 'reestim': 6, 'repeat': 2}, 183: {'exp': 3, 'reestim': 7, 'repeat': 0}, 184: {'exp': 3, 'reestim': 7, 'repeat': 1}, 185: {'exp': 3, 'reestim': 7, 'repeat': 2}, 186: {'exp': 3, 'reestim': 8, 'repeat': 0}, 187: {'exp': 3, 'reestim': 8, 'repeat': 1}, 188: {'exp': 3, 'reestim': 8, 'repeat': 2}, 189: {'exp': 3, 'reestim': 9, 'repeat': 0}, 190: {'exp': 3, 'reestim': 9, 'repeat': 1}, 191: {'exp': 3, 'reestim': 9, 'repeat': 2}, 192: {'exp': 3, 'reestim': 10, 'repeat': 0}, 193: {'exp': 3, 'reestim': 10, 'repeat': 1}, 194: {'exp': 3, 'reestim': 10, 'repeat': 2}, 195: {'exp': 3, 'reestim': 11, 'repeat': 0}, 196: {'exp': 3, 'reestim': 11, 'repeat': 1}, 197: {'exp': 3, 'reestim': 11, 'repeat': 2}, 198: {'exp': 3, 'reestim': 12, 'repeat': 0}, 199: {'exp': 3, 'reestim': 12, 'repeat': 1}, 200: {'exp': 3, 'reestim': 12, 'repeat': 2}, 201: {'exp': 3, 'reestim': 13, 'repeat': 0}, 202: {'exp': 3, 'reestim': 13, 'repeat': 1}, 203: {'exp': 3, 'reestim': 13, 'repeat': 2}, 204: {'exp': 3, 'reestim': 14, 'repeat': 0}, 205: {'exp': 3, 'reestim': 14, 'repeat': 1}, 206: {'exp': 3, 'reestim': 14, 'repeat': 2}, 207: {'exp': 3, 'reestim': 15, 'repeat': 0}, 208: {'exp': 3, 'reestim': 15, 'repeat': 1}, 209: {'exp': 3, 'reestim': 15, 'repeat': 2}, 210: {'exp': 3, 'reestim': 16, 'repeat': 0}, 211: {'exp': 3, 'reestim': 16, 'repeat': 1}, 212: {'exp': 3, 'reestim': 16, 'repeat': 2}, 213: {'exp': 3, 'reestim': 17, 'repeat': 0}, 214: {'exp': 3, 'reestim': 17, 'repeat': 1}, 215: {'exp': 3, 'reestim': 17, 'repeat': 2}, 216: {'exp': 3, 'reestim': 18, 'repeat': 0}, 217: {'exp': 3, 'reestim': 18, 'repeat': 1}, 218: {'exp': 3, 'reestim': 18, 'repeat': 2}, 219: {'exp': 3, 'reestim': 19, 'repeat': 0}, 220: {'exp': 3, 'reestim': 19, 'repeat': 1}, 221: {'exp': 3, 'reestim': 19, 'repeat': 2}, 222: {'exp': 3, 'reestim': 20, 'repeat': 0}, 223: {'exp': 3, 'reestim': 20, 'repeat': 1}, 224: {'exp': 3, 'reestim': 20, 'repeat': 2}, 225: {'exp': 3, 'reestim': 21, 'repeat': 0}, 226: {'exp': 3, 'reestim': 21, 'repeat': 1}, 227: {'exp': 3, 'reestim': 21, 'repeat': 2}, 228: {'exp': 3, 'reestim': 22, 'repeat': 0}, 229: {'exp': 3, 'reestim': 22, 'repeat': 1}, 230: {'exp': 3, 'reestim': 22, 'repeat': 2}, 231: {'exp': 3, 'reestim': 23, 'repeat': 0}, 232: {'exp': 3, 'reestim': 23, 'repeat': 1}, 233: {'exp': 3, 'reestim': 23, 'repeat': 2}, 234: {'exp': 3, 'reestim': 24, 'repeat': 0}, 235: {'exp': 3, 'reestim': 24, 'repeat': 1}, 236: {'exp': 3, 'reestim': 24, 'repeat': 2}, 237: {'exp': 3, 'reestim': 25, 'repeat': 0}, 238: {'exp': 3, 'reestim': 25, 'repeat': 1}, 239: {'exp': 3, 'reestim': 25, 'repeat': 2}, 240: {'exp': 3, 'reestim': 26, 'repeat': 0}, 241: {'exp': 3, 'reestim': 26, 'repeat': 1}, 242: {'exp': 3, 'reestim': 26, 'repeat': 2}, 243: {'exp': 3, 'reestim': 27, 'repeat': 0}, 244: {'exp': 3, 'reestim': 27, 'repeat': 1}, 245: {'exp': 3, 'reestim': 27, 'repeat': 2}, 246: {'exp': 3, 'reestim': 28, 'repeat': 0}, 247: {'exp': 3, 'reestim': 28, 'repeat': 1}, 248: {'exp': 3, 'reestim': 28, 'repeat': 2}, 249: {'exp': 3, 'reestim': 29, 'repeat': 0}, 250: {'exp': 3, 'reestim': 29, 'repeat': 1}, 251: {'exp': 3, 'reestim': 29, 'repeat': 2}, 252: {'exp': 3, 'reestim': 30, 'repeat': 0}, 253: {'exp': 3, 'reestim': 30, 'repeat': 1}, 254: {'exp': 3, 'reestim': 30, 'repeat': 2}, 255: {'exp': 3, 'reestim': 31, 'repeat': 0}, 256: {'exp': 3, 'reestim': 31, 'repeat': 1}, 257: {'exp': 3, 'reestim': 31, 'repeat': 2}, 258: {'exp': 3, 'reestim': 32, 'repeat': 0}, 259: {'exp': 3, 'reestim': 32, 'repeat': 1}, 260: {'exp': 3, 'reestim': 32, 'repeat': 2}, 261: {'exp': 3, 'reestim': 33, 'repeat': 0}, 262: {'exp': 3, 'reestim': 33, 'repeat': 1}, 263: {'exp': 3, 'reestim': 33, 'repeat': 2}, 264: {'exp': 3, 'reestim': 34, 'repeat': 0}, 265: {'exp': 3, 'reestim': 34, 'repeat': 1}, 266: {'exp': 3, 'reestim': 34, 'repeat': 2}, 267: {'exp': 3, 'reestim': 35, 'repeat': 0}, 268: {'exp': 3, 'reestim': 35, 'repeat': 1}, 269: {'exp': 3, 'reestim': 35, 'repeat': 2}, 270: {'exp': 4, 'reestim': 0, 'repeat': 0}, 271: {'exp': 4, 'reestim': 0, 'repeat': 1}, 272: {'exp': 4, 'reestim': 0, 'repeat': 2}, 273: {'exp': 4, 'reestim': 1, 'repeat': 0}, 274: {'exp': 4, 'reestim': 1, 'repeat': 1}, 275: {'exp': 4, 'reestim': 1, 'repeat': 2}, 276: {'exp': 4, 'reestim': 2, 'repeat': 0}, 277: {'exp': 4, 'reestim': 2, 'repeat': 1}, 278: {'exp': 4, 'reestim': 2, 'repeat': 2}, 279: {'exp': 4, 'reestim': 3, 'repeat': 0}, 280: {'exp': 4, 'reestim': 3, 'repeat': 1}, 281: {'exp': 4, 'reestim': 3, 'repeat': 2}, 282: {'exp': 4, 'reestim': 4, 'repeat': 0}, 283: {'exp': 4, 'reestim': 4, 'repeat': 1}, 284: {'exp': 4, 'reestim': 4, 'repeat': 2}, 285: {'exp': 4, 'reestim': 5, 'repeat': 0}, 286: {'exp': 4, 'reestim': 5, 'repeat': 1}, 287: {'exp': 4, 'reestim': 5, 'repeat': 2}, 288: {'exp': 4, 'reestim': 6, 'repeat': 0}, 289: {'exp': 4, 'reestim': 6, 'repeat': 1}, 290: {'exp': 4, 'reestim': 6, 'repeat': 2}, 291: {'exp': 4, 'reestim': 7, 'repeat': 0}, 292: {'exp': 4, 'reestim': 7, 'repeat': 1}, 293: {'exp': 4, 'reestim': 7, 'repeat': 2}, 294: {'exp': 4, 'reestim': 8, 'repeat': 0}, 295: {'exp': 4, 'reestim': 8, 'repeat': 1}, 296: {'exp': 4, 'reestim': 8, 'repeat': 2}, 297: {'exp': 4, 'reestim': 9, 'repeat': 0}, 298: {'exp': 4, 'reestim': 9, 'repeat': 1}, 299: {'exp': 4, 'reestim': 9, 'repeat': 2}, 300: {'exp': 4, 'reestim': 10, 'repeat': 0}, 301: {'exp': 4, 'reestim': 10, 'repeat': 1}, 302: {'exp': 4, 'reestim': 10, 'repeat': 2}, 303: {'exp': 4, 'reestim': 11, 'repeat': 0}, 304: {'exp': 4, 'reestim': 11, 'repeat': 1}, 305: {'exp': 4, 'reestim': 11, 'repeat': 2}, 306: {'exp': 4, 'reestim': 12, 'repeat': 0}, 307: {'exp': 4, 'reestim': 12, 'repeat': 1}, 308: {'exp': 4, 'reestim': 12, 'repeat': 2}, 309: {'exp': 4, 'reestim': 13, 'repeat': 0}, 310: {'exp': 4, 'reestim': 13, 'repeat': 1}, 311: {'exp': 4, 'reestim': 13, 'repeat': 2}, 312: {'exp': 4, 'reestim': 14, 'repeat': 0}, 313: {'exp': 4, 'reestim': 14, 'repeat': 1}, 314: {'exp': 4, 'reestim': 14, 'repeat': 2}, 315: {'exp': 4, 'reestim': 15, 'repeat': 0}, 316: {'exp': 4, 'reestim': 15, 'repeat': 1}, 317: {'exp': 4, 'reestim': 15, 'repeat': 2}, 318: {'exp': 4, 'reestim': 16, 'repeat': 0}, 319: {'exp': 4, 'reestim': 16, 'repeat': 1}, 320: {'exp': 4, 'reestim': 16, 'repeat': 2}, 321: {'exp': 4, 'reestim': 17, 'repeat': 0}, 322: {'exp': 4, 'reestim': 17, 'repeat': 1}, 323: {'exp': 4, 'reestim': 17, 'repeat': 2}, 324: {'exp': 4, 'reestim': 18, 'repeat': 0}, 325: {'exp': 4, 'reestim': 18, 'repeat': 1}, 326: {'exp': 4, 'reestim': 18, 'repeat': 2}, 327: {'exp': 4, 'reestim': 19, 'repeat': 0}, 328: {'exp': 4, 'reestim': 19, 'repeat': 1}, 329: {'exp': 4, 'reestim': 19, 'repeat': 2}, 330: {'exp': 4, 'reestim': 20, 'repeat': 0}, 331: {'exp': 4, 'reestim': 20, 'repeat': 1}, 332: {'exp': 4, 'reestim': 20, 'repeat': 2}, 333: {'exp': 4, 'reestim': 21, 'repeat': 0}, 334: {'exp': 4, 'reestim': 21, 'repeat': 1}, 335: {'exp': 4, 'reestim': 21, 'repeat': 2}, 336: {'exp': 4, 'reestim': 22, 'repeat': 0}, 337: {'exp': 4, 'reestim': 22, 'repeat': 1}, 338: {'exp': 4, 'reestim': 22, 'repeat': 2}, 339: {'exp': 4, 'reestim': 23, 'repeat': 0}, 340: {'exp': 4, 'reestim': 23, 'repeat': 1}, 341: {'exp': 4, 'reestim': 23, 'repeat': 2}, 342: {'exp': 4, 'reestim': 24, 'repeat': 0}, 343: {'exp': 4, 'reestim': 24, 'repeat': 1}, 344: {'exp': 4, 'reestim': 24, 'repeat': 2}, 345: {'exp': 4, 'reestim': 25, 'repeat': 0}, 346: {'exp': 4, 'reestim': 25, 'repeat': 1}, 347: {'exp': 4, 'reestim': 25, 'repeat': 2}, 348: {'exp': 4, 'reestim': 26, 'repeat': 0}, 349: {'exp': 4, 'reestim': 26, 'repeat': 1}, 350: {'exp': 4, 'reestim': 26, 'repeat': 2}, 351: {'exp': 4, 'reestim': 27, 'repeat': 0}, 352: {'exp': 4, 'reestim': 27, 'repeat': 1}, 353: {'exp': 4, 'reestim': 27, 'repeat': 2}, 354: {'exp': 4, 'reestim': 28, 'repeat': 0}, 355: {'exp': 4, 'reestim': 28, 'repeat': 1}, 356: {'exp': 4, 'reestim': 28, 'repeat': 2}, 357: {'exp': 4, 'reestim': 29, 'repeat': 0}, 358: {'exp': 4, 'reestim': 29, 'repeat': 1}, 359: {'exp': 4, 'reestim': 29, 'repeat': 2}, 360: {'exp': 4, 'reestim': 30, 'repeat': 0}, 361: {'exp': 4, 'reestim': 30, 'repeat': 1}, 362: {'exp': 4, 'reestim': 30, 'repeat': 2}, 363: {'exp': 4, 'reestim': 31, 'repeat': 0}, 364: {'exp': 4, 'reestim': 31, 'repeat': 1}, 365: {'exp': 4, 'reestim': 31, 'repeat': 2}, 366: {'exp': 4, 'reestim': 32, 'repeat': 0}, 367: {'exp': 4, 'reestim': 32, 'repeat': 1}, 368: {'exp': 4, 'reestim': 32, 'repeat': 2}, 369: {'exp': 4, 'reestim': 33, 'repeat': 0}, 370: {'exp': 4, 'reestim': 33, 'repeat': 1}, 371: {'exp': 4, 'reestim': 33, 'repeat': 2}, 372: {'exp': 4, 'reestim': 34, 'repeat': 0}, 373: {'exp': 4, 'reestim': 34, 'repeat': 1}, 374: {'exp': 4, 'reestim': 34, 'repeat': 2}, 375: {'exp': 4, 'reestim': 35, 'repeat': 0}, 376: {'exp': 4, 'reestim': 35, 'repeat': 1}, 377: {'exp': 4, 'reestim': 35, 'repeat': 2}, 378: {'exp': 5, 'reestim': 0, 'repeat': 0}, 379: {'exp': 5, 'reestim': 0, 'repeat': 1}, 380: {'exp': 5, 'reestim': 0, 'repeat': 2}, 381: {'exp': 5, 'reestim': 1, 'repeat': 0}, 382: {'exp': 5, 'reestim': 1, 'repeat': 1}, 383: {'exp': 5, 'reestim': 1, 'repeat': 2}, 384: {'exp': 5, 'reestim': 2, 'repeat': 0}, 385: {'exp': 5, 'reestim': 2, 'repeat': 1}, 386: {'exp': 5, 'reestim': 2, 'repeat': 2}, 387: {'exp': 5, 'reestim': 3, 'repeat': 0}, 388: {'exp': 5, 'reestim': 3, 'repeat': 1}, 389: {'exp': 5, 'reestim': 3, 'repeat': 2}, 390: {'exp': 5, 'reestim': 4, 'repeat': 0}, 391: {'exp': 5, 'reestim': 4, 'repeat': 1}, 392: {'exp': 5, 'reestim': 4, 'repeat': 2}, 393: {'exp': 5, 'reestim': 5, 'repeat': 0}, 394: {'exp': 5, 'reestim': 5, 'repeat': 1}, 395: {'exp': 5, 'reestim': 5, 'repeat': 2}, 396: {'exp': 5, 'reestim': 6, 'repeat': 0}, 397: {'exp': 5, 'reestim': 6, 'repeat': 1}, 398: {'exp': 5, 'reestim': 6, 'repeat': 2}, 399: {'exp': 5, 'reestim': 7, 'repeat': 0}, 400: {'exp': 5, 'reestim': 7, 'repeat': 1}, 401: {'exp': 5, 'reestim': 7, 'repeat': 2}, 402: {'exp': 5, 'reestim': 8, 'repeat': 0}, 403: {'exp': 5, 'reestim': 8, 'repeat': 1}, 404: {'exp': 5, 'reestim': 8, 'repeat': 2}, 405: {'exp': 6, 'reestim': 0, 'repeat': 0}, 406: {'exp': 6, 'reestim': 0, 'repeat': 1}, 407: {'exp': 6, 'reestim': 0, 'repeat': 2}, 408: {'exp': 6, 'reestim': 1, 'repeat': 0}, 409: {'exp': 6, 'reestim': 1, 'repeat': 1}, 410: {'exp': 6, 'reestim': 1, 'repeat': 2}, 411: {'exp': 6, 'reestim': 2, 'repeat': 0}, 412: {'exp': 6, 'reestim': 2, 'repeat': 1}, 413: {'exp': 6, 'reestim': 2, 'repeat': 2}, 414: {'exp': 6, 'reestim': 3, 'repeat': 0}, 415: {'exp': 6, 'reestim': 3, 'repeat': 1}, 416: {'exp': 6, 'reestim': 3, 'repeat': 2}, 417: {'exp': 6, 'reestim': 4, 'repeat': 0}, 418: {'exp': 6, 'reestim': 4, 'repeat': 1}, 419: {'exp': 6, 'reestim': 4, 'repeat': 2}, 420: {'exp': 6, 'reestim': 5, 'repeat': 0}, 421: {'exp': 6, 'reestim': 5, 'repeat': 1}, 422: {'exp': 6, 'reestim': 5, 'repeat': 2}, 423: {'exp': 6, 'reestim': 6, 'repeat': 0}, 424: {'exp': 6, 'reestim': 6, 'repeat': 1}, 425: {'exp': 6, 'reestim': 6, 'repeat': 2}, 426: {'exp': 6, 'reestim': 7, 'repeat': 0}, 427: {'exp': 6, 'reestim': 7, 'repeat': 1}, 428: {'exp': 6, 'reestim': 7, 'repeat': 2}, 429: {'exp': 6, 'reestim': 8, 'repeat': 0}, 430: {'exp': 6, 'reestim': 8, 'repeat': 1}, 431: {'exp': 6, 'reestim': 8, 'repeat': 2}}
Instance 0, {'exp': 0, 'reestim': 0, 'repeat': 0}
Run _load_params(): Load params for only Expeirment 0
Folder at results/qtr_14nov_test already exists
DataLoader: Loaded dataset quarterly_new
Experiment Initialized: experiment 0, repeat 0, reestim 0, num reestims 9
Experiment load_results(): Loading results for Experiment id 0, reestims: 9, repeats_to_include: None
Experiment load_results(): Not trained yet
Experiment Initialized: experiment RF, repeat 0, reestim 0, num reestims 36
Experiment load_results(): Loading results for Experiment id RF, reestims: 36, repeats_to_include: None
Experiment load_results(): Not trained yet
Experiment Initialized: experiment XGBoost, repeat 0, reestim 0, num reestims 36
Experiment load_results(): Loading results for Experiment id XGBoost, reestims: 36, repeats_to_include: None
Experiment load_results(): Not trained yet
DataProcesser: Bootstraps fixed
Size of x_mat before appending Nonlinear (241, 16)
Size of x_mat_marx (241, 32)
x_mat_all size (241, 48)
x_mat_all size (241, 48)
Size of X_train afer appending time (241, 108) Time dummy setting: 2
x_pos {'house_starts': [0, 4, 8, 12], 'unrate': [1, 5, 9, 13], 'GDP': [2, 6, 10, 14], 'inf': [3, 7, 11, 15]}
Size of X_train (169, 108)
Endog: 48, Exog: 0, Time: 60
s_pos [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]]
Bootstrap iteration 0 at time 2022-11-15 00:42:18.266594
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.026580810546875, MSE: 6.8845534324646
Epoch: 0, Loss: 10.374665260314941, OOB Loss: 8.154967308044434, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.895411729812622, MSE: 1.7164348363876343
Epoch: 40, Loss: 1.480846881866455, OOB Loss: 0.34706711769104004, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.693868160247803, MSE: 1.6189111471176147
Epoch: 80, Loss: -0.2949042320251465, OOB Loss: 0.6469821929931641, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 52, train loss: 0.5576126575469971, best OOB loss: 0.13351917266845703, LR: 0.0001927001541133519
Bootstrap iteration 1 at time 2022-11-15 00:42:31.097981
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.6436262130737305, MSE: 5.9733500480651855
Epoch: 0, Loss: 8.293136596679688, OOB Loss: 7.693473815917969, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.252806663513184, MSE: 2.367084264755249
Epoch: 40, Loss: 0.3625664710998535, OOB Loss: 0.6029438972473145, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.1728949546813965, MSE: 2.5456185340881348
Epoch: 80, Loss: -1.4595704078674316, OOB Loss: 1.6555414199829102, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 53, train loss: -0.31995272636413574, best OOB loss: 0.39905452728271484, LR: 0.00019221840372806852
Bootstrap iteration 2 at time 2022-11-15 00:42:42.412676
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.921278953552246, MSE: 11.626094818115234
Epoch: 0, Loss: 9.016489028930664, OOB Loss: 9.284460067749023, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.907655715942383, MSE: 4.114083766937256
Epoch: 40, Loss: 0.3093714714050293, OOB Loss: 4.482619762420654, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 4.194296836853027, MSE: 3.5373246669769287
Epoch: 80, Loss: -1.585240364074707, OOB Loss: 4.852210998535156, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 29, train loss: 1.046189308166504, best OOB loss: 3.8759965896606445, LR: 0.00020411985997349918
Bootstrap iteration 3 at time 2022-11-15 00:42:51.122845
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.29252815246582, MSE: 10.072786331176758
Epoch: 0, Loss: 9.11636734008789, OOB Loss: 8.636360168457031, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.71916127204895, MSE: 2.7693917751312256
Epoch: 40, Loss: 0.745582103729248, OOB Loss: 1.1202874183654785, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.013163089752197, MSE: 2.556133270263672
Epoch: 80, Loss: -1.0081768035888672, OOB Loss: 0.7549428939819336, LR: 0.00020411985997349918, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.469397068023682, MSE: 2.5820178985595703
Epoch: 120, Loss: -2.603783130645752, OOB Loss: 1.8226666450500488, LR: 0.00018467216305461778, precision_lambda: 0.01
Early stopped, best epoch: 74, train loss: -1.0001568794250488, best OOB loss: 0.6721243858337402, LR: 0.00018237527420765127
Bootstrap iteration 4 at time 2022-11-15 00:43:04.967804
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.700325012207031, MSE: 10.63271713256836
Epoch: 0, Loss: 9.08599853515625, OOB Loss: 8.688129425048828, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.65728497505188, MSE: 3.0328173637390137
Epoch: 40, Loss: 0.5905826091766357, OOB Loss: 1.9850332736968994, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.837411880493164, MSE: 2.809799909591675
Epoch: 80, Loss: -1.5040090084075928, OOB Loss: 3.347836494445801, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 43, train loss: 0.6360948085784912, best OOB loss: 1.9756066799163818, LR: 0.00019709060505473087
Bootstrap iteration 5 at time 2022-11-15 00:43:15.346080
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.134707450866699, MSE: 17.7520751953125
Epoch: 0, Loss: 9.115947723388672, OOB Loss: 10.170223236083984, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.7399282455444336, MSE: 4.380012035369873
Epoch: 40, Loss: -0.03510785102844238, OOB Loss: 4.444860458374023, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 4.393734455108643, MSE: 4.078834533691406
Epoch: 80, Loss: -1.9154026508331299, OOB Loss: 4.569667339324951, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 32, train loss: 0.6912341117858887, best OOB loss: 3.8500406742095947, LR: 0.00020259278508169968
Bootstrap iteration 6 at time 2022-11-15 00:43:24.013573
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.511009216308594, MSE: 5.361276626586914
Epoch: 0, Loss: 8.307254791259766, OOB Loss: 7.492609024047852, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.295510292053223, MSE: 1.9887601137161255
Epoch: 40, Loss: 0.2827589511871338, OOB Loss: 0.654749870300293, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.062396049499512, MSE: 1.9794247150421143
Epoch: 80, Loss: -1.4269309043884277, OOB Loss: 1.643646240234375, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 49, train loss: -0.3684077262878418, best OOB loss: 0.46309709548950195, LR: 0.0001941526617476871
Bootstrap iteration 7 at time 2022-11-15 00:43:34.307391
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.926591873168945, MSE: 4.83740234375
Epoch: 0, Loss: 9.383648872375488, OOB Loss: 7.79846715927124, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.820284843444824, MSE: 2.008910894393921
Epoch: 40, Loss: 0.5430493354797363, OOB Loss: 0.918726921081543, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.546465873718262, MSE: 2.0289289951324463
Epoch: 80, Loss: -1.0283613204956055, OOB Loss: 1.730882167816162, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 55, train loss: -0.22452068328857422, best OOB loss: 0.8857440948486328, LR: 0.0001912585130744515
Bootstrap iteration 8 at time 2022-11-15 00:43:45.391343
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.482836723327637, MSE: 9.650517463684082
Epoch: 0, Loss: 8.852218627929688, OOB Loss: 8.332069396972656, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.234782695770264, MSE: 3.1157948970794678
Epoch: 40, Loss: 0.4057044982910156, OOB Loss: 2.2807440757751465, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.0380988121032715, MSE: 2.902752161026001
Epoch: 80, Loss: -1.3806343078613281, OOB Loss: 3.840562343597412, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 38, train loss: 0.5549888610839844, best OOB loss: 2.2305221557617188, LR: 0.00019957282318741837
Bootstrap iteration 9 at time 2022-11-15 00:43:54.807493
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.148905277252197, MSE: 11.774006843566895
Epoch: 0, Loss: 8.980426788330078, OOB Loss: 9.150550842285156, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.5182743072509766, MSE: 3.8842899799346924
Epoch: 40, Loss: 0.04367804527282715, OOB Loss: 3.5415682792663574, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 4.537604331970215, MSE: 3.666844129562378
Epoch: 80, Loss: -1.6185922622680664, OOB Loss: 5.165252685546875, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 33, train loss: 0.9486038684844971, best OOB loss: 3.24731183052063, LR: 0.00020208630311899546
Bootstrap iteration 10 at time 2022-11-15 00:44:04.197886
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.370091438293457, MSE: 6.700852870941162
Epoch: 0, Loss: 8.371257781982422, OOB Loss: 7.70562744140625, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.105674743652344, MSE: 3.550771474838257
Epoch: 40, Loss: 0.25569581985473633, OOB Loss: 4.068885803222656, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.782778739929199, MSE: 3.1371755599975586
Epoch: 80, Loss: -2.119380474090576, OOB Loss: 5.659008979797363, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 33, train loss: 0.9246363639831543, best OOB loss: 3.7232537269592285, LR: 0.00020208630311899546
Bootstrap iteration 11 at time 2022-11-15 00:44:13.188055
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.697059631347656, MSE: 11.501631736755371
Epoch: 0, Loss: 8.392435073852539, OOB Loss: 8.70892333984375, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.080580472946167, MSE: 4.3780999183654785
Epoch: 40, Loss: -0.536665678024292, OOB Loss: 4.127101898193359, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.035965919494629, MSE: 4.4041595458984375
Epoch: 80, Loss: -2.3220481872558594, OOB Loss: 6.301568031311035, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 34, train loss: -0.16982102394104004, best OOB loss: 3.7573797702789307, LR: 0.00020158108736119797
Bootstrap iteration 12 at time 2022-11-15 00:44:22.516206
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.2348103523254395, MSE: 10.00446605682373
Epoch: 0, Loss: 9.12809944152832, OOB Loss: 8.927146911621094, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.40427303314209, MSE: 3.1327364444732666
Epoch: 40, Loss: 0.36475515365600586, OOB Loss: 2.4940457344055176, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.5943708419799805, MSE: 3.1176164150238037
Epoch: 80, Loss: -1.1374092102050781, OOB Loss: 4.423407554626465, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 36, train loss: 1.1676814556121826, best OOB loss: 2.422328233718872, LR: 0.000200574441806188
Bootstrap iteration 13 at time 2022-11-15 00:44:32.193420
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.847445487976074, MSE: 8.663047790527344
Epoch: 0, Loss: 8.51677131652832, OOB Loss: 8.797271728515625, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.921546220779419, MSE: 3.2000668048858643
Epoch: 40, Loss: 0.24325346946716309, OOB Loss: 2.1591122150421143, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.920313358306885, MSE: 3.1254515647888184
Epoch: 80, Loss: -1.5276970863342285, OOB Loss: 4.048766613006592, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 39, train loss: 0.0951988697052002, best OOB loss: 2.1407899856567383, LR: 0.00019907389112944982
Bootstrap iteration 14 at time 2022-11-15 00:44:42.291613
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.109604835510254, MSE: 7.3491129875183105
Epoch: 0, Loss: 8.958248138427734, OOB Loss: 8.35561466217041, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.079355716705322, MSE: 2.899413824081421
Epoch: 40, Loss: 0.4236435890197754, OOB Loss: 1.8436779975891113, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.2868266105651855, MSE: 3.0759341716766357
Epoch: 80, Loss: -1.3810062408447266, OOB Loss: 4.2893595695495605, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 34, train loss: 0.7100210189819336, best OOB loss: 1.5535922050476074, LR: 0.00020158108736119797
Bootstrap iteration 15 at time 2022-11-15 00:44:51.703703
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.050532341003418, MSE: 5.0698747634887695
Epoch: 0, Loss: 8.940592765808105, OOB Loss: 7.941890716552734, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.904426097869873, MSE: 1.8173763751983643
Epoch: 40, Loss: 0.8044135570526123, OOB Loss: -0.29479002952575684, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.511790752410889, MSE: 1.6578073501586914
Epoch: 80, Loss: -0.9770236015319824, OOB Loss: -0.45586204528808594, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 65, train loss: -0.841010570526123, best OOB loss: -0.6664166450500488, LR: 0.00018653048465890098
Bootstrap iteration 16 at time 2022-11-15 00:45:04.887626
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.498036861419678, MSE: 4.324329853057861
Epoch: 0, Loss: 8.280601501464844, OOB Loss: 7.380892276763916, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.880108833312988, MSE: 2.5663182735443115
Epoch: 40, Loss: 0.16526508331298828, OOB Loss: 1.4336004257202148, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.401114463806152, MSE: 2.665727138519287
Epoch: 80, Loss: -1.7062633037567139, OOB Loss: 3.471466064453125, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 31, train loss: 0.6763863563537598, best OOB loss: 1.327376365661621, LR: 0.00020310053642275656
Bootstrap iteration 17 at time 2022-11-15 00:45:13.977339
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.389880180358887, MSE: 2.549570083618164
Epoch: 0, Loss: 8.477764129638672, OOB Loss: 6.926060676574707, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 4.677285194396973, MSE: 1.361680269241333
Epoch: 40, Loss: 0.8687753677368164, OOB Loss: -0.618499755859375, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.697343349456787, MSE: 1.2864586114883423
Epoch: 80, Loss: -1.0764191150665283, OOB Loss: -0.14751243591308594, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 52, train loss: -0.0008373260498046875, best OOB loss: -0.6423912048339844, LR: 0.0001927001541133519
Bootstrap iteration 18 at time 2022-11-15 00:45:25.515672
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.552635192871094, MSE: 3.7256569862365723
Epoch: 0, Loss: 9.486176490783691, OOB Loss: 8.101813316345215, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 5.006694793701172, MSE: 1.9055442810058594
Epoch: 40, Loss: 1.3179209232330322, OOB Loss: 0.32999563217163086, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.794017791748047, MSE: 1.8439273834228516
Epoch: 80, Loss: -0.5755152702331543, OOB Loss: 0.8480515480041504, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 57, train loss: 0.05362844467163086, best OOB loss: -0.058428287506103516, LR: 0.00019030341587478598
Bootstrap iteration 19 at time 2022-11-15 00:45:37.543199
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.733475208282471, MSE: 11.900984764099121
Epoch: 0, Loss: 8.262107849121094, OOB Loss: 8.829742431640625, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.500802755355835, MSE: 4.207529067993164
Epoch: 40, Loss: -0.028705120086669922, OOB Loss: 2.2752139568328857, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 4.884346008300781, MSE: 4.294028282165527
Epoch: 80, Loss: -1.7041540145874023, OOB Loss: 3.135003089904785, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 48, train loss: -0.6575865745544434, best OOB loss: 1.8774714469909668, LR: 0.00019463925989743065
Bootstrap iteration 20 at time 2022-11-15 00:45:48.962483
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.885921478271484, MSE: 10.537138938903809
Epoch: 0, Loss: 9.2382230758667, OOB Loss: 8.96509075164795, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.4439990520477295, MSE: 2.96686053276062
Epoch: 40, Loss: 0.7890787124633789, OOB Loss: 1.541332483291626, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.562313556671143, MSE: 3.019850254058838
Epoch: 80, Loss: -0.9201951026916504, OOB Loss: 3.2330269813537598, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 38, train loss: 0.9844207763671875, best OOB loss: 1.4696311950683594, LR: 0.00019957282318741837
Bootstrap iteration 21 at time 2022-11-15 00:45:59.011171
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.521409511566162, MSE: 11.057634353637695
Epoch: 0, Loss: 9.671388626098633, OOB Loss: 8.8449125289917, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 3.608023166656494, MSE: 2.965834140777588
Epoch: 40, Loss: 0.6814517974853516, OOB Loss: 2.4023218154907227, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 5.114729404449463, MSE: 2.8618524074554443
Epoch: 80, Loss: -0.9025635719299316, OOB Loss: 2.9014573097229004, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 54, train loss: -0.29718875885009766, best OOB loss: 2.1425185203552246, LR: 0.00019173785771874836
Bootstrap iteration 22 at time 2022-11-15 00:46:10.782457
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -7.64306116104126, MSE: 5.809044361114502
Epoch: 0, Loss: 9.022565841674805, OOB Loss: 8.585020065307617, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 5.182107925415039, MSE: 2.3662872314453125
Epoch: 40, Loss: 0.6890287399291992, OOB Loss: 1.9413604736328125, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 6.933846473693848, MSE: 2.450932741165161
Epoch: 80, Loss: -1.388671875, OOB Loss: 4.5619916915893555, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 32, train loss: 1.2778489589691162, best OOB loss: 1.3050847053527832, LR: 0.00020259278508169968
Bootstrap iteration 23 at time 2022-11-15 00:46:20.056830
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.846527099609375, MSE: 9.85770034790039
Epoch: 0, Loss: 8.327752113342285, OOB Loss: 8.607380867004395, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.8199779987335205, MSE: 6.378075122833252
Epoch: 40, Loss: -0.49994993209838867, OOB Loss: 4.434484481811523, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 4.484030723571777, MSE: 6.974412441253662
Epoch: 80, Loss: -2.1365067958831787, OOB Loss: 7.116684913635254, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 36, train loss: -0.22477221488952637, best OOB loss: 4.2159929275512695, LR: 0.000200574441806188
Bootstrap iteration 24 at time 2022-11-15 00:46:29.238930
X shape (build_VARNN) torch.Size([169, 108])
Approximate NN size (MB):  31.598876953125
OOB Mean Log Det Precision: -6.968103885650635, MSE: 22.721410751342773
Epoch: 0, Loss: 9.587613105773926, OOB Loss: 11.718360900878906, LR: 0.00024937500000000003, precision_lambda: 0.11
OOB Mean Log Det Precision: 2.361666202545166, MSE: 5.458466529846191
Epoch: 40, Loss: 0.4530763626098633, OOB Loss: 3.9527502059936523, LR: 0.00022561558031503797, precision_lambda: 0.01
OOB Mean Log Det Precision: 3.38779878616333, MSE: 5.483203887939453
Epoch: 80, Loss: -1.5206961631774902, OOB Loss: 4.650411128997803, LR: 0.00020411985997349918, precision_lambda: 0.01
Early stopped, best epoch: 48, train loss: -0.17601537704467773, best OOB loss: 3.6317696571350098, LR: 0.00019463925989743065
Experiment train(): Finished training repeat 0 of experiment 0 at 2022-11-15 00:46:40.301417
Experiment compute_unconditional_irfs(): Experiment has multiple hemispheres / exogenous data / FCN, not training unconditional IRFs
Time 0, 2022-11-15 00:46:40.304806
Time 5, 2022-11-15 00:56:38.616710
Experiment compute_multi_forecasts(): Done with Multiforecasting
Training Benchmarks
Computing RF Benchmark for variable 0, time: 2022-11-15 01:02:37.557724
Computing RF Benchmark for variable 1, time: 2022-11-15 01:02:37.642972
Computing RF Benchmark for variable 2, time: 2022-11-15 01:02:37.725133
Computing RF Benchmark for variable 3, time: 2022-11-15 01:02:37.807791
Benchmarks trained
Training Multi-forecasting Benchmarks
Re-estimation window 0, 2022-11-15 01:02:37.919051
Re-estimation window 1, 2022-11-15 01:02:37.926647
Re-estimation window 2, 2022-11-15 01:02:37.933650
Re-estimation window 3, 2022-11-15 01:02:37.940474
Re-estimation window 4, 2022-11-15 01:02:37.947268
Re-estimation window 5, 2022-11-15 01:02:37.953914
Re-estimation window 6, 2022-11-15 01:02:37.960473
Re-estimation window 7, 2022-11-15 01:02:37.966963
Re-estimation window 8, 2022-11-15 01:02:37.973430
Re-estimation window 9, 2022-11-15 01:02:37.979783
Re-estimation window 10, 2022-11-15 01:02:37.986096
Re-estimation window 11, 2022-11-15 01:02:37.992291
Re-estimation window 12, 2022-11-15 01:02:37.998446
Re-estimation window 13, 2022-11-15 01:02:38.004462
Re-estimation window 14, 2022-11-15 01:02:38.010442
Re-estimation window 15, 2022-11-15 01:02:38.016327
Re-estimation window 16, 2022-11-15 01:02:38.022144
Re-estimation window 17, 2022-11-15 01:02:38.027892
Re-estimation window 18, 2022-11-15 01:02:38.033566
Re-estimation window 19, 2022-11-15 01:02:38.039149
Re-estimation window 20, 2022-11-15 01:02:38.044654
Re-estimation window 21, 2022-11-15 01:02:38.050055
Re-estimation window 22, 2022-11-15 01:02:38.055395
Re-estimation window 23, 2022-11-15 01:02:38.060746
Re-estimation window 24, 2022-11-15 01:02:38.065940
Re-estimation window 25, 2022-11-15 01:02:38.071064
Re-estimation window 26, 2022-11-15 01:02:38.076088
Re-estimation window 27, 2022-11-15 01:02:38.081057
Re-estimation window 28, 2022-11-15 01:02:38.085942
Re-estimation window 29, 2022-11-15 01:02:38.090747
Re-estimation window 30, 2022-11-15 01:02:38.095475
Re-estimation window 31, 2022-11-15 01:02:38.100129
Re-estimation window 32, 2022-11-15 01:02:38.104703
Re-estimation window 33, 2022-11-15 01:02:38.109193
Re-estimation window 34, 2022-11-15 01:02:38.113621
Re-estimation window 35, 2022-11-15 01:02:38.117965
Re-estimation window 36, 2022-11-15 01:02:38.122247
Re-estimation window 37, 2022-11-15 01:02:38.126406
Re-estimation window 38, 2022-11-15 01:02:38.130504
Re-estimation window 39, 2022-11-15 01:02:38.134515
Re-estimation window 40, 2022-11-15 01:02:38.138470
Re-estimation window 41, 2022-11-15 01:02:38.142351
Re-estimation window 42, 2022-11-15 01:02:38.146142
Re-estimation window 43, 2022-11-15 01:02:38.149842
Re-estimation window 44, 2022-11-15 01:02:38.153481
Re-estimation window 45, 2022-11-15 01:02:38.157033
Re-estimation window 46, 2022-11-15 01:02:38.160504
Re-estimation window 47, 2022-11-15 01:02:38.163885
Re-estimation window 48, 2022-11-15 01:02:38.167188
Re-estimation window 49, 2022-11-15 01:02:38.170426
Re-estimation window 50, 2022-11-15 01:02:38.173600
Re-estimation window 51, 2022-11-15 01:02:38.176689
Re-estimation window 52, 2022-11-15 01:02:38.179704
Re-estimation window 53, 2022-11-15 01:02:38.182639
Re-estimation window 54, 2022-11-15 01:02:38.185515
Re-estimation window 55, 2022-11-15 01:02:38.188292
Re-estimation window 56, 2022-11-15 01:02:38.190978
Re-estimation window 57, 2022-11-15 01:02:38.193591
Re-estimation window 58, 2022-11-15 01:02:38.196119
Re-estimation window 59, 2022-11-15 01:02:38.198571
Re-estimation window 60, 2022-11-15 01:02:38.200942
Re-estimation window 61, 2022-11-15 01:02:38.203238
Re-estimation window 62, 2022-11-15 01:02:38.205454
Re-estimation window 63, 2022-11-15 01:02:38.207599
Re-estimation window 64, 2022-11-15 01:02:38.209662
Re-estimation window 65, 2022-11-15 01:02:38.211655
Re-estimation window 66, 2022-11-15 01:02:38.213571
Re-estimation window 67, 2022-11-15 01:02:38.215398
Re-estimation window 68, 2022-11-15 01:02:38.217138
Re-estimation window 69, 2022-11-15 01:02:38.218787
Re-estimation window 70, 2022-11-15 01:02:38.220359
Re-estimation window 71, 2022-11-15 01:02:38.221836
Re-estimation window 0, 2022-11-15 01:02:38.225659
Re-estimation window 1, 2022-11-15 01:02:38.232600
Re-estimation window 2, 2022-11-15 01:02:38.239417
Re-estimation window 3, 2022-11-15 01:02:38.246166
Re-estimation window 4, 2022-11-15 01:02:38.252801
Re-estimation window 5, 2022-11-15 01:02:38.259388
Re-estimation window 6, 2022-11-15 01:02:38.265909
Re-estimation window 7, 2022-11-15 01:02:38.272323
Re-estimation window 8, 2022-11-15 01:02:38.278676
Re-estimation window 9, 2022-11-15 01:02:38.284987
Re-estimation window 10, 2022-11-15 01:02:38.291178
Re-estimation window 11, 2022-11-15 01:02:38.297286
Re-estimation window 12, 2022-11-15 01:02:38.303312
Re-estimation window 13, 2022-11-15 01:02:38.309254
Re-estimation window 14, 2022-11-15 01:02:38.315111
Re-estimation window 15, 2022-11-15 01:02:38.320898
Re-estimation window 16, 2022-11-15 01:02:38.326635
Re-estimation window 17, 2022-11-15 01:02:38.332267
Re-estimation window 18, 2022-11-15 01:02:38.337824
Re-estimation window 19, 2022-11-15 01:02:38.343304
Re-estimation window 20, 2022-11-15 01:02:38.348694
Re-estimation window 21, 2022-11-15 01:02:38.354025
Re-estimation window 22, 2022-11-15 01:02:38.359271
Re-estimation window 23, 2022-11-15 01:02:38.364419
Re-estimation window 24, 2022-11-15 01:02:38.369514
Re-estimation window 25, 2022-11-15 01:02:38.374536
Re-estimation window 26, 2022-11-15 01:02:38.379477
Re-estimation window 27, 2022-11-15 01:02:38.384353
Re-estimation window 28, 2022-11-15 01:02:38.389129
Re-estimation window 29, 2022-11-15 01:02:38.393801
Re-estimation window 30, 2022-11-15 01:02:38.398401
Re-estimation window 31, 2022-11-15 01:02:38.402903
Re-estimation window 32, 2022-11-15 01:02:38.407333
Re-estimation window 33, 2022-11-15 01:02:38.411683
Re-estimation window 34, 2022-11-15 01:02:38.415974
Re-estimation window 35, 2022-11-15 01:02:38.420166
Re-estimation window 36, 2022-11-15 01:02:38.424310
Re-estimation window 37, 2022-11-15 01:02:38.428358
Re-estimation window 38, 2022-11-15 01:02:38.432321
Re-estimation window 39, 2022-11-15 01:02:38.436202
Re-estimation window 40, 2022-11-15 01:02:38.439999
Re-estimation window 41, 2022-11-15 01:02:38.443723
Re-estimation window 42, 2022-11-15 01:02:38.447367
Re-estimation window 43, 2022-11-15 01:02:38.450914
Re-estimation window 44, 2022-11-15 01:02:38.454407
Re-estimation window 45, 2022-11-15 01:02:38.457818
Re-estimation window 46, 2022-11-15 01:02:38.461141
Re-estimation window 47, 2022-11-15 01:02:38.464380
Re-estimation window 48, 2022-11-15 01:02:38.467547
Re-estimation window 49, 2022-11-15 01:02:38.470630
Re-estimation window 50, 2022-11-15 01:02:38.473653
Re-estimation window 51, 2022-11-15 01:02:38.476590
Re-estimation window 52, 2022-11-15 01:02:38.479441
Re-estimation window 53, 2022-11-15 01:02:38.482209
Re-estimation window 54, 2022-11-15 01:02:38.484892
Re-estimation window 55, 2022-11-15 01:02:38.487540
Re-estimation window 56, 2022-11-15 01:02:38.490070
Re-estimation window 57, 2022-11-15 01:02:38.492529
Re-estimation window 58, 2022-11-15 01:02:38.494894
Re-estimation window 59, 2022-11-15 01:02:38.497184
Re-estimation window 60, 2022-11-15 01:02:38.499403
Re-estimation window 61, 2022-11-15 01:02:38.501538
Re-estimation window 62, 2022-11-15 01:02:38.503588
Re-estimation window 63, 2022-11-15 01:02:38.505567
Re-estimation window 64, 2022-11-15 01:02:38.507467
Re-estimation window 65, 2022-11-15 01:02:38.509282
Re-estimation window 66, 2022-11-15 01:02:38.511013
Re-estimation window 67, 2022-11-15 01:02:38.512669
Re-estimation window 68, 2022-11-15 01:02:38.514227
Re-estimation window 69, 2022-11-15 01:02:38.515705
Re-estimation window 70, 2022-11-15 01:02:38.517099
Re-estimation window 71, 2022-11-15 01:02:38.518411
Re-estimation window 0, 2022-11-15 01:02:38.522098
Re-estimation window 1, 2022-11-15 01:02:38.536587
Re-estimation window 2, 2022-11-15 01:02:38.550269
Re-estimation window 3, 2022-11-15 01:02:38.563813
Re-estimation window 4, 2022-11-15 01:02:38.577425
Re-estimation window 5, 2022-11-15 01:02:38.590926
Re-estimation window 6, 2022-11-15 01:02:38.604686
Re-estimation window 7, 2022-11-15 01:02:38.618025
Re-estimation window 8, 2022-11-15 01:02:38.631256
Re-estimation window 9, 2022-11-15 01:02:38.644311
Re-estimation window 10, 2022-11-15 01:02:38.657308
Re-estimation window 11, 2022-11-15 01:02:38.670188
Re-estimation window 12, 2022-11-15 01:02:38.682973
Re-estimation window 13, 2022-11-15 01:02:38.695738
Re-estimation window 14, 2022-11-15 01:02:38.708388
Re-estimation window 15, 2022-11-15 01:02:38.720886
Re-estimation window 16, 2022-11-15 01:02:38.733316
Re-estimation window 17, 2022-11-15 01:02:38.745673
Re-estimation window 18, 2022-11-15 01:02:38.757904
Re-estimation window 19, 2022-11-15 01:02:38.770088
Re-estimation window 20, 2022-11-15 01:02:38.782266
Re-estimation window 21, 2022-11-15 01:02:38.794279
Re-estimation window 22, 2022-11-15 01:02:38.806171
Re-estimation window 23, 2022-11-15 01:02:38.817972
Re-estimation window 24, 2022-11-15 01:02:38.829664
Re-estimation window 25, 2022-11-15 01:02:38.841273
Re-estimation window 26, 2022-11-15 01:02:38.852810
Re-estimation window 27, 2022-11-15 01:02:38.864269
Re-estimation window 28, 2022-11-15 01:02:38.875634
Re-estimation window 29, 2022-11-15 01:02:38.886890
Re-estimation window 30, 2022-11-15 01:02:38.898071
Re-estimation window 31, 2022-11-15 01:02:38.909158
Re-estimation window 32, 2022-11-15 01:02:38.920134
Re-estimation window 33, 2022-11-15 01:02:38.931066
Re-estimation window 34, 2022-11-15 01:02:38.941867
Re-estimation window 35, 2022-11-15 01:02:38.952607
Re-estimation window 36, 2022-11-15 01:02:38.963230
Re-estimation window 37, 2022-11-15 01:02:38.973964
Re-estimation window 38, 2022-11-15 01:02:38.984450
Re-estimation window 39, 2022-11-15 01:02:38.994843
Re-estimation window 40, 2022-11-15 01:02:39.005109
Re-estimation window 41, 2022-11-15 01:02:39.015318
Re-estimation window 42, 2022-11-15 01:02:39.025363
Re-estimation window 43, 2022-11-15 01:02:39.035332
Re-estimation window 44, 2022-11-15 01:02:39.045195
Re-estimation window 45, 2022-11-15 01:02:39.054982
Re-estimation window 46, 2022-11-15 01:02:39.064678
Re-estimation window 47, 2022-11-15 01:02:39.074259
Re-estimation window 48, 2022-11-15 01:02:39.083746
Re-estimation window 49, 2022-11-15 01:02:39.093173
Re-estimation window 50, 2022-11-15 01:02:39.102494
Re-estimation window 51, 2022-11-15 01:02:39.111758
Re-estimation window 52, 2022-11-15 01:02:39.120892
Re-estimation window 53, 2022-11-15 01:02:39.129955
Re-estimation window 54, 2022-11-15 01:02:39.138891
Re-estimation window 55, 2022-11-15 01:02:39.147775
Re-estimation window 56, 2022-11-15 01:02:39.156521
Re-estimation window 57, 2022-11-15 01:02:39.165168
Re-estimation window 58, 2022-11-15 01:02:39.173729
Re-estimation window 59, 2022-11-15 01:02:39.182216
Re-estimation window 60, 2022-11-15 01:02:39.190618
Re-estimation window 61, 2022-11-15 01:02:39.198962
Re-estimation window 62, 2022-11-15 01:02:39.207133
Re-estimation window 63, 2022-11-15 01:02:39.215213
Re-estimation window 64, 2022-11-15 01:02:39.223208
Re-estimation window 65, 2022-11-15 01:02:39.231116
Re-estimation window 66, 2022-11-15 01:02:39.238958
Re-estimation window 67, 2022-11-15 01:02:39.246681
Re-estimation window 68, 2022-11-15 01:02:39.254343
Re-estimation window 69, 2022-11-15 01:02:39.261911
Re-estimation window 70, 2022-11-15 01:02:39.269366
Re-estimation window 71, 2022-11-15 01:02:39.276722
Re-estimation window 0, 2022-11-15 01:02:39.286633
Re-estimation window 1, 2022-11-15 01:02:39.300503
Re-estimation window 2, 2022-11-15 01:02:39.314123
Re-estimation window 3, 2022-11-15 01:02:39.327688
Re-estimation window 4, 2022-11-15 01:02:39.341160
Re-estimation window 5, 2022-11-15 01:02:39.354544
Re-estimation window 6, 2022-11-15 01:02:39.367799
Re-estimation window 7, 2022-11-15 01:02:39.380950
Re-estimation window 8, 2022-11-15 01:02:39.394014
Re-estimation window 9, 2022-11-15 01:02:39.407024
Re-estimation window 10, 2022-11-15 01:02:39.419934
Re-estimation window 11, 2022-11-15 01:02:39.432744
Re-estimation window 12, 2022-11-15 01:02:39.445467
Re-estimation window 13, 2022-11-15 01:02:39.458077
Re-estimation window 14, 2022-11-15 01:02:39.470616
Re-estimation window 15, 2022-11-15 01:02:39.483077
Re-estimation window 16, 2022-11-15 01:02:39.495459
Re-estimation window 17, 2022-11-15 01:02:39.507801
Re-estimation window 18, 2022-11-15 01:02:39.520030
Re-estimation window 19, 2022-11-15 01:02:39.532195
Re-estimation window 20, 2022-11-15 01:02:39.544197
Re-estimation window 21, 2022-11-15 01:02:39.556135
Re-estimation window 22, 2022-11-15 01:02:39.567965
Re-estimation window 23, 2022-11-15 01:02:39.579733
Re-estimation window 24, 2022-11-15 01:02:39.591375
Re-estimation window 25, 2022-11-15 01:02:39.602955
Re-estimation window 26, 2022-11-15 01:02:39.614402
Re-estimation window 27, 2022-11-15 01:02:39.625741
Re-estimation window 28, 2022-11-15 01:02:39.636967
Re-estimation window 29, 2022-11-15 01:02:39.648111
Re-estimation window 30, 2022-11-15 01:02:39.659183
Re-estimation window 31, 2022-11-15 01:02:39.670159
Re-estimation window 32, 2022-11-15 01:02:39.681056
Re-estimation window 33, 2022-11-15 01:02:39.691847
Re-estimation window 34, 2022-11-15 01:02:39.702578
Re-estimation window 35, 2022-11-15 01:02:39.713175
Re-estimation window 36, 2022-11-15 01:02:39.723688
Re-estimation window 37, 2022-11-15 01:02:39.734183
Re-estimation window 38, 2022-11-15 01:02:39.744532
Re-estimation window 39, 2022-11-15 01:02:39.754779
Re-estimation window 40, 2022-11-15 01:02:39.764939
Re-estimation window 41, 2022-11-15 01:02:39.775003
Re-estimation window 42, 2022-11-15 01:02:39.784955
Re-estimation window 43, 2022-11-15 01:02:39.794841
Re-estimation window 44, 2022-11-15 01:02:39.804668
Re-estimation window 45, 2022-11-15 01:02:39.814350
Re-estimation window 46, 2022-11-15 01:02:39.823931
Re-estimation window 47, 2022-11-15 01:02:39.833449
Re-estimation window 48, 2022-11-15 01:02:39.842862
Re-estimation window 49, 2022-11-15 01:02:39.852168
Re-estimation window 50, 2022-11-15 01:02:39.861377
Re-estimation window 51, 2022-11-15 01:02:39.870529
Re-estimation window 52, 2022-11-15 01:02:39.879562
Re-estimation window 53, 2022-11-15 01:02:39.888510
Re-estimation window 54, 2022-11-15 01:02:39.897372
Re-estimation window 55, 2022-11-15 01:02:39.906187
Re-estimation window 56, 2022-11-15 01:02:39.914907
Re-estimation window 57, 2022-11-15 01:02:39.923530
Re-estimation window 58, 2022-11-15 01:02:39.932020
Re-estimation window 59, 2022-11-15 01:02:39.940396
Re-estimation window 60, 2022-11-15 01:02:39.948706
Re-estimation window 61, 2022-11-15 01:02:39.956870
Re-estimation window 62, 2022-11-15 01:02:39.964964
Re-estimation window 63, 2022-11-15 01:02:39.973541
Re-estimation window 64, 2022-11-15 01:02:39.981450
Re-estimation window 65, 2022-11-15 01:02:39.989262
Re-estimation window 66, 2022-11-15 01:02:39.996981
Re-estimation window 67, 2022-11-15 01:02:40.004612
Re-estimation window 68, 2022-11-15 01:02:40.012139
Re-estimation window 69, 2022-11-15 01:02:40.041326
Re-estimation window 70, 2022-11-15 01:02:40.049779
Re-estimation window 71, 2022-11-15 01:02:40.057002
Multi-forecasting Benchmarks trained
{'show_train': 3, 'opt_bootstrap': 2, 'epochs': 400, 'num_bootstrap': 25, 'sampling_rate': 0.75, 'block_size': 8, 'cancel_out': False, 'standardize': True, 'prior_shift': False, 'prior_shift_strength': 0, 'oob_loss_multiple_threshold': 5, 'save_models': False, 'exog': None, 'exog_data': False, 'tol': 0.0001, 'nodes': [200, 100, 50], 'tvpl_archi': [5], 'constant_tvpl': [100], 'patience': 50, 'lr': 0.0005, 'lr_multiple': 0.9975, 'dropout_rate': 0.25, 'actv': 'nn.ReLU()', 'time_dummy_setting': 2, 'marx': True, 'dummy_interval': 4, 'l1_input_lambda': 0, 'l0_input_lambda': 0, 'input_dropout_rate': 0, 'vsn': False, 'fcn': False, 'eqn_by_eqn': False, 'time_hemi_prior_variance': 1, 'vol_hemi_prior_variance': 1, 'fix_bootstrap': True, 'loss_weight_param': 0.5, 'log_det_multiple': 1, 'precision_lambda': 0.05, 'end_precision_lambda': 0.01, 'lambda_temper_epochs': 25, 'optimizer': 'Adam', 'n_lag_d': 8, 'n_lag_linear': 4, 'n_lag_ps': 1, 'variables': ['DGS3', 'inf', 'unrate'], 's_pos_setting': {'hemis': 'endog'}, 'joint_estimation': True, 's_pos': [[0, 9], [9, 70]], 'name': 'Baseline', 'test_size': 72, 'dataset': 'quarterly_new', 'num_inner_bootstraps': 25, 'num_repeats': 3, 'default_nn_hyps': 'nn_hyps_default', 'var_names': ['house_starts', 'unrate', 'GDP', 'inf'], 'n_var': 4, 'reestim_params': {'reestim': True, 'same_train': False, 'reestimation_window': 2}, 'folder_path': 'results/qtr_14nov_test', 'image_folder_path': 'results/qtr_14nov_test/images', 'model': 'RF', 'same_train': False, 'max_test_size': 72}
Size of x_mat before appending Nonlinear (241, 16)
Size of x_mat_marx (241, 32)
x_mat_all size (241, 48)
x_mat_all size (241, 48)
Size of X_train afer appending time (241, 108) Time dummy setting: 2
x_pos {'house_starts': [0, 4, 8, 12], 'unrate': [1, 5, 9, 13], 'GDP': [2, 6, 10, 14], 'inf': [3, 7, 11, 15]}
Size of X_train (169, 108)
Endog: 48, Exog: 0, Time: 60
s_pos [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]
Standardizing
Variable 0, best hyps: {'n_estimators': 50, 'max_depth': 3}, time: 2022-11-15 01:02:45.329805
Variable 1, best hyps: {'n_estimators': 100, 'max_depth': 9}, time: 2022-11-15 01:02:50.832285
Variable 2, best hyps: {'n_estimators': 25, 'max_depth': 9}, time: 2022-11-15 01:02:56.292097
Variable 3, best hyps: {'n_estimators': 50, 'max_depth': 3}, time: 2022-11-15 01:03:01.430213
100 2022-11-15 01:03:02.780767
200 2022-11-15 01:03:04.060844
300 2022-11-15 01:03:05.340849
400 2022-11-15 01:03:06.620581
500 2022-11-15 01:03:07.901933
100 2022-11-15 01:03:10.479118
200 2022-11-15 01:03:11.759342
300 2022-11-15 01:03:13.043462
400 2022-11-15 01:03:14.329043
500 2022-11-15 01:03:15.613327
100 2022-11-15 01:03:18.168291
200 2022-11-15 01:03:19.451533
300 2022-11-15 01:03:20.736663
400 2022-11-15 01:03:22.022714
500 2022-11-15 01:03:23.308269
100 2022-11-15 01:03:25.889196
200 2022-11-15 01:03:27.179311
300 2022-11-15 01:03:28.462684
400 2022-11-15 01:03:29.746674
500 2022-11-15 01:03:31.027522
100 2022-11-15 01:03:33.586485
200 2022-11-15 01:03:34.871352
300 2022-11-15 01:03:36.151909
400 2022-11-15 01:03:37.434036
500 2022-11-15 01:03:38.717872
100 2022-11-15 01:03:41.298011
200 2022-11-15 01:03:42.582334
300 2022-11-15 01:03:43.872021
400 2022-11-15 01:03:45.156453
500 2022-11-15 01:03:46.445336
100 2022-11-15 01:03:48.995216
200 2022-11-15 01:03:50.275630
300 2022-11-15 01:03:51.553157
400 2022-11-15 01:03:52.834022
500 2022-11-15 01:03:54.117494
100 2022-11-15 01:03:56.694676
200 2022-11-15 01:03:57.977603
300 2022-11-15 01:03:59.260184
400 2022-11-15 01:04:00.544947
500 2022-11-15 01:04:01.830054
Time 0, 2022-11-15 01:04:03.118858
Done with Multiforecasting
Experiment _compile_results(): Multiple Jobs, compiling turned off
Experiment _compile_unconditional_irf_results(): Multiple Jobs, compiling turned off
Experiment _compile_multi_forecasting_results(): Multiple Jobs, compiling turned off
{'show_train': 3, 'opt_bootstrap': 2, 'epochs': 400, 'num_bootstrap': 25, 'sampling_rate': 0.75, 'block_size': 8, 'cancel_out': False, 'standardize': True, 'prior_shift': False, 'prior_shift_strength': 0, 'oob_loss_multiple_threshold': 5, 'save_models': False, 'exog': None, 'exog_data': False, 'tol': 0.0001, 'nodes': [200, 100, 50], 'tvpl_archi': [5], 'constant_tvpl': [100], 'patience': 50, 'lr': 0.0005, 'lr_multiple': 0.9975, 'dropout_rate': 0.25, 'actv': 'nn.ReLU()', 'time_dummy_setting': 2, 'marx': True, 'dummy_interval': 4, 'l1_input_lambda': 0, 'l0_input_lambda': 0, 'input_dropout_rate': 0, 'vsn': False, 'fcn': False, 'eqn_by_eqn': False, 'time_hemi_prior_variance': 1, 'vol_hemi_prior_variance': 1, 'fix_bootstrap': True, 'loss_weight_param': 0.5, 'log_det_multiple': 1, 'precision_lambda': 0.05, 'end_precision_lambda': 0.01, 'lambda_temper_epochs': 25, 'optimizer': 'Adam', 'n_lag_d': 8, 'n_lag_linear': 4, 'n_lag_ps': 1, 'variables': ['DGS3', 'inf', 'unrate'], 's_pos_setting': {'hemis': 'endog'}, 'joint_estimation': True, 's_pos': [[0, 9], [9, 70]], 'name': 'Baseline', 'test_size': 72, 'dataset': 'quarterly_new', 'num_inner_bootstraps': 25, 'num_repeats': 3, 'default_nn_hyps': 'nn_hyps_default', 'var_names': ['house_starts', 'unrate', 'GDP', 'inf'], 'n_var': 4, 'reestim_params': {'reestim': True, 'same_train': False, 'reestimation_window': 2}, 'folder_path': 'results/qtr_14nov_test', 'image_folder_path': 'results/qtr_14nov_test/images', 'model': 'XGBoost', 'same_train': False, 'max_test_size': 72}
Size of x_mat before appending Nonlinear (241, 16)
Size of x_mat_marx (241, 32)
x_mat_all size (241, 48)
x_mat_all size (241, 48)
Size of X_train afer appending time (241, 108) Time dummy setting: 2
x_pos {'house_starts': [0, 4, 8, 12], 'unrate': [1, 5, 9, 13], 'GDP': [2, 6, 10, 14], 'inf': [3, 7, 11, 15]}
Size of X_train (169, 108)
Endog: 48, Exog: 0, Time: 60
s_pos [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]
Standardizing
Variable 0, best hyps: {'n_estimators': 50, 'max_depth': 3}, time: 2022-11-15 01:04:14.631927
Variable 1, best hyps: {'n_estimators': 100, 'max_depth': 9}, time: 2022-11-15 01:04:20.139337
Variable 2, best hyps: {'n_estimators': 25, 'max_depth': 9}, time: 2022-11-15 01:04:25.566894
Variable 3, best hyps: {'n_estimators': 50, 'max_depth': 3}, time: 2022-11-15 01:04:30.696752
100 2022-11-15 01:04:32.042743
200 2022-11-15 01:04:33.321759
300 2022-11-15 01:04:34.603184
400 2022-11-15 01:04:35.885263
500 2022-11-15 01:04:37.168829
100 2022-11-15 01:04:39.721717
200 2022-11-15 01:04:41.022255
300 2022-11-15 01:04:42.300326
400 2022-11-15 01:04:43.577145
500 2022-11-15 01:04:44.859134
100 2022-11-15 01:04:47.414928
200 2022-11-15 01:04:48.698482
300 2022-11-15 01:04:49.985264
400 2022-11-15 01:04:51.268979
500 2022-11-15 01:04:52.553838
100 2022-11-15 01:04:55.126570
200 2022-11-15 01:04:56.402718
300 2022-11-15 01:04:57.682490
400 2022-11-15 01:04:58.968417
500 2022-11-15 01:05:00.251617
100 2022-11-15 01:05:02.812655
200 2022-11-15 01:05:04.104542
300 2022-11-15 01:05:05.390562
400 2022-11-15 01:05:06.670031
500 2022-11-15 01:05:07.954237
100 2022-11-15 01:05:10.540770
200 2022-11-15 01:05:11.821545
300 2022-11-15 01:05:13.100600
400 2022-11-15 01:05:14.380564
500 2022-11-15 01:05:15.661700
100 2022-11-15 01:05:18.213575
200 2022-11-15 01:05:19.494171
300 2022-11-15 01:05:20.780756
400 2022-11-15 01:05:22.065613
500 2022-11-15 01:05:23.351932
100 2022-11-15 01:05:25.931589
200 2022-11-15 01:05:27.211037
300 2022-11-15 01:05:28.493433
400 2022-11-15 01:05:29.778245
500 2022-11-15 01:05:31.064023
Time 0, 2022-11-15 01:05:32.351919
Done with Multiforecasting
Experiment _compile_results(): Multiple Jobs, compiling turned off
Experiment _compile_unconditional_irf_results(): Multiple Jobs, compiling turned off
Experiment _compile_multi_forecasting_results(): Multiple Jobs, compiling turned off
